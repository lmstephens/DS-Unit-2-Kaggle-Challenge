{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LS_DS_222.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UEHyIgGaaSEJ"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mRfPLX4WgLVJ"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRRNhkxcgLVK"
      },
      "source": [
        "- use scikit-learn for **random forests**\n",
        "- do **ordinal encoding** with high-cardinality categoricals\n",
        "- understand how categorical encodings affect trees differently compared to linear models\n",
        "- understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-3TH11e1gLVL"
      },
      "source": [
        "Today's lesson has two take-away messages:\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models. \n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r5PbOFEuFfGF"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries\n",
        "\n",
        "- **category_encoders** \n",
        "- **graphviz**\n",
        "- ipywidgets\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- seaborn\n",
        "- scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FStAplyRFoEu",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZL-yK8B7gLVW"
      },
      "source": [
        "# Use scikit-learn for random forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKW9JZv5qpTG",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let's fit a Random Forest!\n",
        "\n",
        "![](https://pbs.twimg.com/media/EGSvKA0UUAEzUZi?format=png)\n",
        "\n",
        "[Chris Albon, MachineLearningFlashcards.com](https://twitter.com/chrisalbon/status/1181261589887909889)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHFxMCPSgLVM"
      },
      "source": [
        "### Solution example\n",
        "\n",
        "First, read & wrangle the data.\n",
        "\n",
        "> Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. (For example, [what other columns have zeros and shouldn't?](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values) What other columns are duplicates, or nearly duplicates? Can you extract the year from date_recorded? Can you engineer new features, such as the number of years from waterpump construction to waterpump inspection?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTLm-rDagLVM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', \n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col+'_MISSING'] = X[col].isnull()\n",
        "            \n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    X = X.drop(columns=duplicates)\n",
        "    \n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "    \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "    \n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "    X['years_MISSING'] = X['years'].isnull()\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m2HppBvZgLVP",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target\n",
        "train_features = train.drop(columns=[target])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aXmK2brXgLVR",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxRSdsImqpTS",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "[Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "57yyygsdgLVW",
        "outputId": "294bbca4-8b22-416a-ede0-927d7e448802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%%time\n",
        "# WARNING: the %%time command sometimes has quirks/bugs\n",
        "\n",
        "import category_encoders as ce\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "  ce.OneHotEncoder(use_cat_names=True),\n",
        "  SimpleImputer(strategy='median'),\n",
        "  RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        ")\n",
        "# fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8088383838383838\n",
            "CPU times: user 25 s, sys: 512 ms, total: 25.6 s\n",
            "Wall time: 16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhYtGmB7GXwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da67dabc-c6ef-4738-b32f-11c543dea2f0"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47520, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYNx4wkIGg1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "051d530c-2850-429d-ff0e-4af87809fa14"
      },
      "source": [
        "print('X_train shape before encoding', X_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape before encoding (47520, 38)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPzVCfbQG-iP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdcfc980-0790-46f8-ff2d-6a9c60a86413"
      },
      "source": [
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded = encoder.transform(X_train)\n",
        "print('X_train shape after encoding', encoded.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape after encoding (47520, 182)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYMHwpKTHz3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "543f4034-1bd1-49b7-b6c0-55b21ed4566e"
      },
      "source": [
        "encoded"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>amount_tsh</th>\n",
              "      <th>gps_height</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>num_private</th>\n",
              "      <th>region_code</th>\n",
              "      <th>district_code</th>\n",
              "      <th>population</th>\n",
              "      <th>construction_year</th>\n",
              "      <th>year_recorded</th>\n",
              "      <th>month_recorded</th>\n",
              "      <th>day_recorded</th>\n",
              "      <th>years</th>\n",
              "      <th>basin_Lake Nyasa</th>\n",
              "      <th>basin_Rufiji</th>\n",
              "      <th>basin_Wami / Ruvu</th>\n",
              "      <th>basin_Lake Victoria</th>\n",
              "      <th>basin_Internal</th>\n",
              "      <th>basin_Lake Tanganyika</th>\n",
              "      <th>basin_Pangani</th>\n",
              "      <th>basin_Lake Rukwa</th>\n",
              "      <th>basin_Ruvuma / Southern Coast</th>\n",
              "      <th>region_Mbeya</th>\n",
              "      <th>region_Iringa</th>\n",
              "      <th>region_Pwani</th>\n",
              "      <th>region_Kagera</th>\n",
              "      <th>region_Dodoma</th>\n",
              "      <th>region_Rukwa</th>\n",
              "      <th>region_Arusha</th>\n",
              "      <th>region_Mwanza</th>\n",
              "      <th>region_Mtwara</th>\n",
              "      <th>region_Tanga</th>\n",
              "      <th>region_Kilimanjaro</th>\n",
              "      <th>region_Manyara</th>\n",
              "      <th>region_Lindi</th>\n",
              "      <th>region_Kigoma</th>\n",
              "      <th>region_Morogoro</th>\n",
              "      <th>region_Shinyanga</th>\n",
              "      <th>region_Ruvuma</th>\n",
              "      <th>region_Tabora</th>\n",
              "      <th>...</th>\n",
              "      <th>quantity_unknown</th>\n",
              "      <th>source_spring</th>\n",
              "      <th>source_shallow well</th>\n",
              "      <th>source_machine dbh</th>\n",
              "      <th>source_river</th>\n",
              "      <th>source_hand dtw</th>\n",
              "      <th>source_lake</th>\n",
              "      <th>source_rainwater harvesting</th>\n",
              "      <th>source_dam</th>\n",
              "      <th>source_other</th>\n",
              "      <th>source_unknown</th>\n",
              "      <th>source_type_spring</th>\n",
              "      <th>source_type_shallow well</th>\n",
              "      <th>source_type_borehole</th>\n",
              "      <th>source_type_river/lake</th>\n",
              "      <th>source_type_rainwater harvesting</th>\n",
              "      <th>source_type_dam</th>\n",
              "      <th>source_type_other</th>\n",
              "      <th>source_class_groundwater</th>\n",
              "      <th>source_class_surface</th>\n",
              "      <th>source_class_unknown</th>\n",
              "      <th>waterpoint_type_communal standpipe</th>\n",
              "      <th>waterpoint_type_hand pump</th>\n",
              "      <th>waterpoint_type_other</th>\n",
              "      <th>waterpoint_type_communal standpipe multiple</th>\n",
              "      <th>waterpoint_type_improved spring</th>\n",
              "      <th>waterpoint_type_cattle trough</th>\n",
              "      <th>waterpoint_type_dam</th>\n",
              "      <th>waterpoint_type_group_communal standpipe</th>\n",
              "      <th>waterpoint_type_group_hand pump</th>\n",
              "      <th>waterpoint_type_group_other</th>\n",
              "      <th>waterpoint_type_group_improved spring</th>\n",
              "      <th>waterpoint_type_group_cattle trough</th>\n",
              "      <th>waterpoint_type_group_dam</th>\n",
              "      <th>longitude_MISSING</th>\n",
              "      <th>latitude_MISSING</th>\n",
              "      <th>construction_year_MISSING</th>\n",
              "      <th>gps_height_MISSING</th>\n",
              "      <th>population_MISSING</th>\n",
              "      <th>years_MISSING</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43360</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.542898</td>\n",
              "      <td>-9.174777</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7263</th>\n",
              "      <td>500.0</td>\n",
              "      <td>2049.0</td>\n",
              "      <td>34.665760</td>\n",
              "      <td>-9.308548</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>175.0</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2486</th>\n",
              "      <td>25.0</td>\n",
              "      <td>290.0</td>\n",
              "      <td>38.238568</td>\n",
              "      <td>-6.179919</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2300.0</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.716727</td>\n",
              "      <td>-1.289055</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52726</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.389331</td>\n",
              "      <td>-6.399942</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9795</th>\n",
              "      <td>50.0</td>\n",
              "      <td>489.0</td>\n",
              "      <td>38.268574</td>\n",
              "      <td>-5.450254</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1300.0</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>28</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58170</th>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.926294</td>\n",
              "      <td>-9.641293</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2011</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17191</th>\n",
              "      <td>0.0</td>\n",
              "      <td>599.0</td>\n",
              "      <td>39.262924</td>\n",
              "      <td>-10.768079</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>33</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>2013</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8192</th>\n",
              "      <td>30.0</td>\n",
              "      <td>426.0</td>\n",
              "      <td>39.348550</td>\n",
              "      <td>-10.642069</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>33</td>\n",
              "      <td>320.0</td>\n",
              "      <td>1988.0</td>\n",
              "      <td>2013</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49783</th>\n",
              "      <td>50.0</td>\n",
              "      <td>501.0</td>\n",
              "      <td>37.562148</td>\n",
              "      <td>-6.888409</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>2011</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47520 rows Ã— 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       amount_tsh  gps_height  ...  population_MISSING  years_MISSING\n",
              "43360         0.0         NaN  ...                True           True\n",
              "7263        500.0      2049.0  ...               False          False\n",
              "2486         25.0       290.0  ...               False          False\n",
              "313           0.0         NaN  ...                True           True\n",
              "52726         0.0         NaN  ...                True           True\n",
              "...           ...         ...  ...                 ...            ...\n",
              "9795         50.0       489.0  ...               False          False\n",
              "58170         0.0         NaN  ...                True           True\n",
              "17191         0.0       599.0  ...               False          False\n",
              "8192         30.0       426.0  ...               False          False\n",
              "49783        50.0       501.0  ...               False          False\n",
              "\n",
              "[47520 rows x 182 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yfyk_aa5gLVY"
      },
      "source": [
        "# Do ordinal encoding with high-cardinality categoricals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUINhQUmqpTY",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "http://contrib.scikit-learn.org/categorical-encoding/ordinal.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2qUL5PXqpTY",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8d_WJtcgLVZ",
        "colab": {}
      },
      "source": [
        "# Re-arrange data into X features matrix and y target vector, so \n",
        "# we use *all* features, including the high-cardinality categoricals\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fODTIt4_43Zw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7164a10b-8530-4d5c-e28a-fbbd06f19d67"
      },
      "source": [
        "%%time\n",
        "\n",
        "# This pipeline is identical to the example cell above,\n",
        "# except we're replacing one-hot encoder with \"orginal\" encoder\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='median'),\n",
        "    RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy 0.8092592592592592\n",
            "CPU times: user 18.7 s, sys: 149 ms, total: 18.8 s\n",
            "Wall time: 10.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5E_wo9V6wlc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b3c429b-a545-4227-9c22-878225c24114"
      },
      "source": [
        "print('X_train shape before encoding', X_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape before encoding (47520, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx5vXBKP7B7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50c58bf9-1b00-4f6b-de91-6eb173e76d70"
      },
      "source": [
        "encoder = pipeline.named_steps['ordinalencoder']\n",
        "encoded = encoder.transform(X_train)\n",
        "print('X_train shape after encoding', encoded.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape after encoding (47520, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_g3eNldC3pG",
        "colab_type": "text"
      },
      "source": [
        "Coefficents vs Feature Importances\n",
        "\n",
        "\n",
        "*   **Alike:** Both tell you the **strength** of the relationship between an individual feature and the target. (Assuming the features in the linear model are scaled/standardized.)\n",
        "*   **Diffrent** Coefficients can be negative or positive, which tells you the **direction** of the relationship between an individual feature and the target. Feature importances are always positive, don't tell you the direction of the relationship, because that concept doesn't even make sense for a tree-based model which can fit non-linear, *non-monotonic* relationships.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBWWF6olBg-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "40e24919-ddf9-4299-f6b4-e36fd7ca8b37"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances\n",
        "rf = pipeline.named_steps['randomforestclassifier']\n",
        "#rf.feature_importances_\n",
        "# no coefficants rf.coef_\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, encoded.columns)\n",
        "\n",
        "# Plot top n feature importances\n",
        "n = 20\n",
        "plt.figure(figsize=(10, n/2))\n",
        "plt.title(f'Top (n) features')\n",
        "importances.sort_values()[-n:].plot.barh();"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xs2UPoVdgLVp"
      },
      "source": [
        "# Understand how categorical encodings affect trees differently compared to linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIS8BUoFqpTd",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z8V-A92mgLVp"
      },
      "source": [
        "### Categorical exploration, 1 feature at a time\n",
        "\n",
        "Change `feature`, then re-run these cells!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G35RAzVdgLVq",
        "colab": {}
      },
      "source": [
        "feature = 'extraction_type_class'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OuxHWiH8gLVr",
        "colab": {}
      },
      "source": [
        "X_train[feature].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVxoC4NngLVt",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.barplot(\n",
        "    x=train[feature], \n",
        "    y=train['status_group']=='functional', \n",
        "    color='grey'\n",
        ");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w99mek14gLVv",
        "colab": {}
      },
      "source": [
        "X_train[feature].head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezzK2IdbgLVx"
      },
      "source": [
        "### [One Hot Encoding](http://contrib.scikit-learn.org/categorical-encoding/onehot.html)\n",
        "\n",
        "> Onehot (or dummy) coding for categorical features, produces one feature per category, each binary.\n",
        "\n",
        "Warning: May run slow, or run out of memory, with high cardinality categoricals!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HDQZtV6GgLVy",
        "colab": {}
      },
      "source": [
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'{len(encoded.columns)} columns')\n",
        "encoded.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Ql9Qmw3sNJ7"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mT4A-oDGpOss",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lr = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    StandardScaler(), \n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EbH6wivpsRuV"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b6KUluFOqIdK",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True), \n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8yg11_gTsUu6"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IxHwXGRornNI",
        "colab": {}
      },
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0REZ8HdpsccR"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gV-grmYKpDp9",
        "colab": {}
      },
      "source": [
        "# Plot tree\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=7, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QUd6gzcZgLVz"
      },
      "source": [
        "### [Ordinal Encoding](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html)\n",
        "\n",
        "> Ordinal encoding uses a single column of integers to represent the classes. An optional mapping dict can be passed in; in this case, we use the knowledge that there is some true order to the classes themselves. Otherwise, the classes are assumed to have no true order and integers are selected at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CnBz2RbwgLVz",
        "colab": {}
      },
      "source": [
        "encoder = ce.OrdinalEncoder()\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'1 column, {encoded[feature].nunique()} unique values')\n",
        "encoded.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nd-ZWprasqUM"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GJ1YpwjvrhfL",
        "colab": {}
      },
      "source": [
        "lr = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(), \n",
        "    StandardScaler(), \n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9lO_R3SksuHs"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOELD_roriVI",
        "colab": {}
      },
      "source": [
        "dt = make_pipeline(\n",
        "    ce.OrdinalEncoder(), \n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7V2zHjiwswTg"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9UPYPois8QR",
        "colab": {}
      },
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MvmmvE8fsymh"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCvUu4Oms88b",
        "colab": {}
      },
      "source": [
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=5, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4EJi2GvgLVa"
      },
      "source": [
        "# Understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iZSfxsfqpUR",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0nNABF3HgLVg"
      },
      "source": [
        "### What's \"random\" about random forests?\n",
        "1. Each tree trains on a random bootstrap sample of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the `bootstrap` parameter's default is `True`.) This type of ensembling is called Bagging. (Bootstrap AGGregatING.)\n",
        "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.) \n",
        "\n",
        "For extra randomness, you can try [\"extremely randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)!\n",
        "\n",
        ">In extremely randomized trees (see [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOxjZJJqpUT",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pUYP619CgLVb"
      },
      "source": [
        "### Example: [predicting golf putts](https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/)\n",
        "(1 feature, non-linear, regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4640ukxgLVc",
        "colab": {}
      },
      "source": [
        "putts = pd.DataFrame(\n",
        "    columns=['distance', 'tries', 'successes'], \n",
        "    data = [[2, 1443, 1346],\n",
        "            [3, 694, 577],\n",
        "            [4, 455, 337],\n",
        "            [5, 353, 208],\n",
        "            [6, 272, 149],\n",
        "            [7, 256, 136],\n",
        "            [8, 240, 111],\n",
        "            [9, 217, 69],\n",
        "            [10, 200, 67],\n",
        "            [11, 237, 75],\n",
        "            [12, 202, 52],\n",
        "            [13, 192, 46],\n",
        "            [14, 174, 54],\n",
        "            [15, 167, 28],\n",
        "            [16, 201, 27],\n",
        "            [17, 195, 31],\n",
        "            [18, 191, 33],\n",
        "            [19, 147, 20],\n",
        "            [20, 152, 24]]\n",
        ")\n",
        "\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T0IpCcKggLVd",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def putt_trees(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeRegressor(max_depth=max_depth), \n",
        "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
        "    \n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(putts_X, putts_y)\n",
        "        ax = putts.plot('distance', 'rate of success', kind='scatter', title=name)\n",
        "        ax.step(putts_X, model.predict(putts_X), where='mid')\n",
        "        plt.show()\n",
        "        \n",
        "interact(putt_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_KgZK9_9gLVh"
      },
      "source": [
        "### Bagging demo, with golf putts data\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vA9mrSTNgLVi",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
        "def diy_bagging(max_depth=1, n_estimators=1):\n",
        "    y_preds = []\n",
        "    for i in range(n_estimators):\n",
        "        title = f'Tree {i+1}'\n",
        "        bootstrap_sample = putts.sample(n=len(putts), replace=True).sort_values(by='distance')\n",
        "        bootstrap_X = bootstrap_sample[['distance']]\n",
        "        bootstrap_y = bootstrap_sample['rate of success']\n",
        "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "        tree.fit(bootstrap_X, bootstrap_y)\n",
        "        y_pred = tree.predict(bootstrap_X)\n",
        "        y_preds.append(y_pred)\n",
        "        ax = bootstrap_sample.plot('distance', 'rate of success', kind='scatter', title=title)\n",
        "        ax.step(bootstrap_X, y_pred, where='mid')\n",
        "        plt.show()\n",
        "        \n",
        "    ensembled = np.vstack(y_preds).mean(axis=0)\n",
        "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title=title)\n",
        "    ax.step(putts_X, ensembled, where='mid')\n",
        "    plt.show()\n",
        "    \n",
        "interact(diy_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rq4Z_wQ_gLVj"
      },
      "source": [
        "### Go back to Tanzania Waterpumps ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoSE9iT6YXQz"
      },
      "source": [
        "#### Helper function to visualize predicted probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HzIAjGpJgLVj",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import seaborn as sns\n",
        "\n",
        "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : scikit-learn classifier, already fit\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 numeric features\n",
        "    class_index : integer, index of class label\n",
        "    title : string, title of plot\n",
        "    num : int, number of grid points for each feature\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    y_pred_proba : numpy array, predicted probabilities for class_index\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(max2, min2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
        "    pred_grid = y_pred_proba.reshape(num, num).T\n",
        "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
        "    sns.heatmap(table, vmin=0, vmax=1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return y_pred_proba\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DiRfPqHjgLVl"
      },
      "source": [
        "### Compare Decision Tree, Random Forest, Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HKkMLXhMgLVl",
        "colab": {}
      },
      "source": [
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'quantity'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def get_X_y(df, feature1, feature2, target):\n",
        "    features = [feature1, feature2]\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "    X = X.fillna(X.median())\n",
        "    X = ce.OrdinalEncoder().fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "def compare_models(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeClassifier(max_depth=max_depth), \n",
        "              RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators), \n",
        "              LogisticRegression(solver='lbfgs', multi_class='auto')]\n",
        "    \n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(X, y)\n",
        "        pred_heatmap(model, X, [feature1, feature2], class_index=0, title=name)\n",
        "\n",
        "X, y = get_X_y(train, feature1, feature2, target='status_group')\n",
        "interact(compare_models, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hOQqjLEDgLVn"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hm4aPgs2gLVn",
        "colab": {}
      },
      "source": [
        "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
        "\n",
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'latitude'\n",
        "\n",
        "def waterpumps_bagging(max_depth=1, n_estimators=1):\n",
        "    predicteds = []\n",
        "    for i in range(n_estimators):\n",
        "        title = f'Tree {i+1}'\n",
        "        bootstrap_sample = train.sample(n=len(train), replace=True)\n",
        "        X, y = get_X_y(bootstrap_sample, feature1, feature2, target='status_group')\n",
        "        tree = DecisionTreeClassifier(max_depth=max_depth)\n",
        "        tree.fit(X, y)\n",
        "        predicted = pred_heatmap(tree, X, [feature1, feature2], class_index=0, title=title)\n",
        "        predicteds.append(predicted)\n",
        "    \n",
        "    ensembled = np.vstack(predicteds).mean(axis=0)\n",
        "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
        "    sns.heatmap(ensembled.reshape(100, 100).T, vmin=0, vmax=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "        \n",
        "interact(waterpumps_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYoSBi15akWP"
      },
      "source": [
        "# Review\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models. \n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    }
  ]
}