{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4zaww_jSwW0",
        "colab_type": "text"
      },
      "source": [
        "### **Decision Trees**\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "Decision trees have high interpretability after being trained you can literally draw them.\n",
        "Splits data on the feature which when split provides highest infomation gain.\n",
        "*  clean data with **outliers and missing values**\n",
        "*  use scikit-learn **pipelines**\n",
        "*  use scikit-learn for **decision trees**\n",
        "*  get and interpret **feature importances** of a tree-based model\n",
        "*  understand why decision trees are useful to model **non-linear, non-monotonic** relationships and **feature interactions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gd8zmkPZKqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5sxnitJccPT",
        "colab_type": "text"
      },
      "source": [
        "### Clean data outliners and missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r45jwl-Ucin4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check Pandas Profiling version\n",
        "import pandas_profiling\n",
        "pandas_profiling.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsKNghK4ch55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New code for Pandas Profiling version 2.4\n",
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(train, minimal=True).to_notebook_iframe()\n",
        "\n",
        "profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6kxq4dRc4ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "    \n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    cols_with_zeros = ['longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "            \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HczXy3Y-ZE7W",
        "colab_type": "text"
      },
      "source": [
        "### Pipelines\n",
        "We can combine steps with pipelines: Encode, Impute, Scale, Fit, Predict!\n",
        "\n",
        "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "\n",
        "*  **Convenience and encapsulation.** You only have to call fit and predict once on \n",
        "your data to fit a whole sequence of estimators.\n",
        "*  **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "*  **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8OvfUyKPJsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000)\n",
        ")\n",
        "# Fit on train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Score on val\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
        "\n",
        "# Predict on test\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_zHBjWWkxzC",
        "colab_type": "text"
      },
      "source": [
        "Get and plot coefficients\n",
        "This is slightly harder when using pipelines.\n",
        "\n",
        "The pipeline doesn't have a .coef_ attribute. But the model inside the pipeline does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2KydJNVkyfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.named_steps['logisticregression']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "plt.figure(figsize=(10,30))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YYnErFKlSXt",
        "colab_type": "text"
      },
      "source": [
        "### Use scikit-learn for decision trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qraRM5_lVlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the only two changes from the previous cell:\n",
        "# Remove StandardScaler (it's not needed or helpful for trees)\n",
        "# Change the model from LogisticRegression to DecisionTreeClassifier\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "# Fit on train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Score on train, val\n",
        "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
        "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
        "\n",
        "# Predict on test \n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT-3GueQlqOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For visualizations\n",
        "model = pipeline.named_steps['decisiontreeclassifier']\n",
        "encoder = pipeline.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val).columns\n",
        "\n",
        "dot_data = export_graphviz(model, \n",
        "                           out_file=None, \n",
        "                           max_depth=3, \n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_, \n",
        "                           impurity=False, \n",
        "                           filled=True, \n",
        "                           proportion=True, \n",
        "                           rounded=True)   \n",
        "display(graphviz.Source(dot_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1uivtUUmzVd",
        "colab_type": "text"
      },
      "source": [
        "**Using two features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERoBd9aumxU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_location = X_train[['longitude', 'latitude']].copy()\n",
        "val_location = X_val[['longitude', 'latitude']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSpt57dzm6ZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dt = make_pipeline(\n",
        "    SimpleImputer(), \n",
        "    DecisionTreeClassifier(max_depth=16, random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(train_location, y_train)\n",
        "print('Decision Tree:')\n",
        "print('Train Accuracy', dt.score(train_location, y_train))\n",
        "print('Validation Accuracy', dt.score(val_location, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_LTaB4bZLvo",
        "colab_type": "text"
      },
      "source": [
        "### Feature importances of a tree-based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xHoB7XnZTTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLEIUFBSpINy",
        "colab_type": "text"
      },
      "source": [
        "### **Random Forests**\n",
        "*   use scikit-learn for **random forests**\n",
        "*   do **ordinal encoding** with high-cardinality categoricals\n",
        "*   understand how categorical encodings affect trees differently compared to linear models\n",
        "*   understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth\n",
        "\n",
        "Two take-away messages:\n",
        "1.   Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "2.  One-hot encoding isn’t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZns6E8zpgf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}